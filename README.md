<!--

__author__ = "Andrew Rechnitzer"

__copyright__ = "Copyright (C) 2018-9 Andrew Rechnitzer"

__license__ = "GFDL"

-->

  

# Plom is PaperLess Open Marking

  

System to generate tests from a small number of similar source versions

  

## Preliminaries

* We assume that the manager has created a small number of versions of the test (say 4) - the system would work just fine with a single version, but some of the features become less useful..

  

* The test versions must all have the same structure - ie the same questions numbers on each page and same number of pages. A question can run over more than 1 page, and indeed pages are grouped into "page groups", which should be though of as a set of (contiguous) pages that comprise a given question.

  

## Set up development environment

* To create a virtual environment in the current directory, run: `python3 -m venv venv`, this will create the `venv` directory and also create directories inside it containing a copy of the Python interpreter, the standard library, and various supporting files.

  

* Once youâ€™ve created a virtual environment, you may activate it. 
  On Windows, run:
   `venv\Scripts\activate.bat`
  On Unix or MacOS, run:
  `source venv/bin/activate`

* We manage required packages with _requirements.txt_ file. Use pip to install all the dependencies of MLP, run:`pip install -r requirements.txt` 
* If any package is added or deleted, run: `pip freeze >requirements.txt` to record package list of the current environment into _requirements.txt_.

    

  

## Start-up and building tests

  

* Manager runs the 00 script which asks for a test name and location to store all the required files. It will also prompt for a class list (in a sensible csv format), and a manager password (for the page server later on). This is also an opportunity to set up some other server options (port numbers and IP address).

  

* Now move to the location and cd into the build directory.

  

* The manager can run the 01 script.

* This asks asks for, test name, number of versions, and the locations of versions (through a file dialog).

* Once that is set, the manager can set the id page(s) - the pages on which the student will write name / student number / section etc.

* The manager also sets the other page groups and chooses how they will be selected for each actual test paper. In particular, each page group can be selected

* 'Fixed' = all tests get version 1

* 'Random' = each test gets a randomly selected version of the pagegroup

* 'Cycle' = the versions will cycle 1,2,3,4,1,2,3,4, etc through the produced papers

* After all confirmed move to the next script.

  
  

* The 02 script (also in build) then uses the specification generated by the previous script to build the actual test papers that are printed and given to students.

* Some reasonably standard python libraries are used to generate QR codes, and to stamp them onto the required pages pulled from the various test versions, and to then assemble each test and save it in the "examsToPrint" directory with names of the form "exam_xxxx.pdf".

* Each page of each test is stamped with 3 qr-codes. 1x code giving the name of the test (eg something like '101Midterm'), and then 2 copies of a code 'txxxxpyyvz' - where xxxx is the number of the test paper, yy is the page number and z is the version from which the page has been taken. These codes enable later scripts to correctly identify the page and file it away appropriately.

* The qr codes are placed, 1 at the top (avoiding likely staple location) and two at the bottom. This can make it easy for subsequent scripts to automatically orient the page.

* The 02 script actually runs a child script which does the heavy lifting. That, in turn, tries to use gnu-parallel to parallelise as much as possible of the big batch jobs.

  
  

* The test papers are now ready for printing and making many students happy.

  

## Reading and processing scanned exams

  

* After this the test has been given and the papers collected, they need to be scanned and turned into one or more pdfs. These pdfs should be placed in the scanAndGroup direction - more precisely in the scannedExams subdirectory.

  

* Now run the 03 script. This script takes the pdfs in scannedExams and cuts them up (using ghostscript) into individual page images which end up in pageImages directory.

* Based on our experiences with [AMC](https://www.auto-multiple-choice.net/) we use [imagemagick](https://www.imagemagick.org/script/index.php) to apply a simple gamma-shift to each page image in order to keep white as white, but to make all other colours darker. This avoids some (but not all) problems generated by students who use very light pencil.

  
  

* Next the 04 script.

* This examines each pageimage and scans its top and bottom fifths for QR-codes using zbarimg (a standard linux package these days).

* The bottom should have 2 qr-codes (a name and a txxxxpyyvz code) while the top should have only have one (a txxxxpyyvz code).

* The qr-codes are recorded and the pages oriented correctly.

* The resulting files are named appropriately and moved into the 'decodedPages' subdirectory. Inside that directory pages are placed in "pageYY/versionZ" subdirectories according to their codes.

* Note that if there are some pages that cannot be identified, then these are placed in a 'problemImage' directory. These can then be identified manually using the 'manualPageIdentifier' script (which fires up a simple pyqt display app).

  
  

* The 05 script now does some basic sanity checks to make sure that the scanned tests are complete and reports back to the user if any tests are incomplete.

  

* The 06 script assembles page groups according to the specification set by the manager (back in the 01 script).

* The resulting pagegroup images may consist of more than 1 page and so are tiled together using imagemagick. eg if a question goes over pages 4,5,6, then the script will tile pages 4,5,6 into a single (wide) page group image.

* At present the scripts have some simple methods to avoid processing files many times (eg if you scan in the first 50 tests and then later scan in another 100) - but this definitely needs improving.

  

* Its now time to fire up the image server and clients - these are really the guts of Plom.

  

## Image server

  

* cd into the imageServer directory. The image server does what its name suggests - it serves up page images to the clients (ie the markers) and then keeps track of the resulting IDs, annotated images and marks.

  

* First run the server setup script. If they have not already set IP address and ports then they can do so now.

  

* Next the userManager script. This allows the manager to set up some accounts for their markers. It can also set the manager password if this hasn't been done yet. The passwords are stored hashed using [passlib](https://passlib.readthedocs.io/en/stable/).

  

* The image_server is split into several pieces (very few of which the manager needs to worry about)

* some database handling code (which should be moved from python's [peewee](http://docs.peewee-orm.com/en/latest/) library to use (perhaps?) Qt's [database library](http://doc.qt.io/qt-5/sql-connecting.html). One for associating student numbers and names with papers, and then another to record marks + annotated pagegroup images.

* a simple 'authority' which verifies passwords and hands out authorisation tokens to the server (which can then be passed on to clients). The tokens are produced using [uuid4](https://docs.python.org/3.6/library/uuid.html).

* the main server which keeps track of who has which file and what information is coming back from the clients.

* messages between clients and the server are handled using the [aiohttp](https://aiohttp.readthedocs.io/en/stable/) library.

* These messages are encrypted using SSL. The 00 script will build a new certificate for the project (providing relevant libraries are installed).

  

* Once the users are set up and the server ports etc are set, just run the image_server script and its time to fire up the clients.

  

* That being said, there are also two manager scripts in the image server directory - these display information from the current databases and allow the manager to filter the data and display some basic statistics. These need more work so that the manager can look at individual papers and (for example)

* put a pageimage back on the TODO pile (ie send it for regrading)

* regrade a page image (rather than sending putting it back on a TAs to-do list)

* reenter the student information

* probably a few more things when we think of them.

  

## Plom Client - Marking and identifing.

* The client consists of 3 parts.

* a start up page - where the client chooses whether to mark or identify. They need to enter some server infor, their username / password and (if marking) the pagegroup and version they are marking.

* the identifier - where the user is shown the id page(s) of tests and enters names and / or student numbers.

* the marker - where the client is shown pagegroups and they can annotate and mark them.

  
  

* The identifier client displays the id pages from a test and the TA needs to enter either the student's ID number or their name.

* The client requests a copy of the class list from the server (which currently must be a csv with headings 'id', 'surname', 'name')

* Having a local copy (stored in a temp directory) of the class list means that both the student number and student name text entry boxes have auto-completion.

* One the TA has identified the current paper, the information is sent back to the server and then a new id page image is downloaded.

  

* The marker client is much more involved - not least because page images must be downloaded from the client and then annotated, marked and uploaded again.

* on start up the marker client asks the server the total marks for the pagegroup/version that the TA has been assigned.

* then the main window opens up - this shows the current test as well as options for annotation, reversion (ie going back to the original pageimage) etc. Initially no pageimages are present and the TA has to click "get next test".

* on clicking this the client requests a groupimage (ie matching the pagegroup + version the TA is marking) from the server.

* now the TA can annotate the paper (by pressing the 'annotate' button or hitting enter). This fires up a separate window with a little annotation application in it.

* The annotation window has several simple paint tools (like pen, line, box, tick, cross). Using left-click with line draws a line, while right-click draws a line with an arrow-head. Left-click with tick/cross produces a tick/cross, while right-click gives a cross/tick. Right-click on pen gives a yellow highlighter.

* There is a simple text tool (which one leaves using the escape key). Additionally the TA can click on a 'standard comment' and then click in the window to paste that comment. Comments in the comment list can be edited, added, deleted etc. They are stored in the 'commentList.json' file.

* There is undo / redo.

* If you click "cancel" then the annotation window is closed with no changes made to the image and the app returns to its main window.

* After annotating, the TA can assign marks in 3 different ways. In each case the final mark is printed in the top-left of the page image.

* Mark total - the TA is presented with numbered buttons from 0 to the max possible. Clicking the relevant button then sets the total.

* Mark up - the total is set to 0 and the TA is presented with numbered buttons (+1, +2,..,+5). Clicking the button doesn't increment the total, but if the TA then clicks on the page-image a circled (+2) (for example) is printed on the page and the total incremented.

* Mark down - the total is set to the max possible, and the TA now has options (-1,-2,..,-5). Clicking on the relevant button and then in the page leaves a circled (-2) (for example) and decrements the total appropriately.

  

* Once the marking / annotation of the current paper is done, clicking finished the mark and page image are saved locally (in a temp directory) and also send back to the server. The next paper is automatically requested and displayed. The TA can then simply hit enter to start annotating.

* An already annotated paper can be "reverted" back to its untouched state, or it can be annotated further. In this second option the previous annotations cannot be undone (they are now part of the background). Perhaps better to "revert" if significant changes needed.

  

## Finishing.

* After all the pageimages are marked and all the papers identified, one should close the servers and move on to finishing.

  

* This stage of the code requires more polishing, but does work.

  

* Run the 07 script to check which tests have been completely graded and identified.

* It also outputs "testMarks.csv" for processing in a spreadsheet, and eventually uploading to Canvas.

  

* The 08 script then builds coverpages for those tests which include the student's information, their scores on the various page-groups, and which version of each pagegroup they did. These are placed in the "coverPages" subdirectory.

  

* The 09 script then reassembles the identified / marked test from its constituent pagegroups and puts the coverpage on the front. The result is a pdf that is placed in the 'reassembled' subdirectory. At present the resulting pdf is renamed as exam_X.pdf where X is the student number.

  

## Returning tests.

* Canvas does not, as yet, allow us to bulk upload documents for students (one for each student). Perhaps a canvas-workaround would be to upload links for students, where the file name for each paper is called something like "exam_X_Y.pdf" where X is the student number and Y is a random (or md5-sum generated) string.

  

* Alternatively, we have been using a system documented in `docs/returning_papers.md` where students get a "return code" from Canvas that they then type into a different webpage to obtain their exam.